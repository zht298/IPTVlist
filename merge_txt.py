# å°†å¤šä¸ªTXTæ–‡ä»¶åˆå¹¶æˆä¸€ä¸ªæ–‡ä»¶ï¼Œå¹¶è¿‡æ»¤æ‰åŒ…å« 'ipv6' çš„è¡Œï¼ŒæŒ‰æŒ‡å®šæ•°é‡ä¿ç•™åˆ†ç»„å†…ç›¸åŒé¢‘é“æ•°é‡ï¼Œä¿ç•™æŒ‡å®šçš„åˆ†ç»„
import requests
from collections import defaultdict
import warnings
import time

# ç¦ç”¨æœªéªŒè¯çš„HTTPSè¯·æ±‚è­¦å‘Š
warnings.filterwarnings('ignore', message='Unverified HTTPS request')

def download_txt_file(url, filename):
    """ä»URLä¸‹è½½TXTæ–‡ä»¶å¹¶ä¿å­˜åœ¨æœ¬åœ°ã€‚"""
    retries = 3
    for attempt in range(retries):
        try:
            response = requests.get(url, verify=False)  # ç»•è¿‡ SSL éªŒè¯
            response.raise_for_status()
            with open(filename, 'wb') as file:
                file.write(response.content)
            return
        except requests.exceptions.SSLError:
            continue
        except requests.exceptions.RequestException:
            continue
        if attempt < retries - 1:
            time.sleep(3)

def merge_txt_files(file_list, output_filename, max_channels_per_name):
    """å°†å¤šä¸ªTXTæ–‡ä»¶åˆå¹¶æˆä¸€ä¸ªæ–‡ä»¶ï¼Œå¹¶è¿‡æ»¤æ‰åŒ…å« 'ipv6' çš„è¡ŒåŠæŒ‰æŒ‡å®šæ•°é‡ä¿ç•™æ¯ä¸ªé¢‘é“åç§°çš„é¡¹ã€‚"""
    group_dict = defaultdict(lambda: defaultdict(list))

    for filename, groups in file_list:
        with open(filename, 'r', encoding='utf-8', errors='ignore') as infile:
            current_group = None
            for line in infile:
                if line.startswith("#") or not line.strip() or 'ipv6' in line.lower():
                    continue
                parts = line.split(',')
                if len(parts) == 2 and parts[1].startswith('#genre#'):
                    current_group = parts[0].strip()
                elif current_group and len(parts) == 2:
                    channel_name, link = parts[0].strip(), parts[1].strip()
                    if groups is None or current_group in groups:
                        group_dict[current_group][channel_name].append(link)

    with open(output_filename, 'w', encoding='utf-8') as outfile:
        for group, channels in group_dict.items():
            outfile.write(f"{group},#genre#\n")
            for channel_name, links in channels.items():
                for link in links[:max_channels_per_name]:
                    outfile.write(f"{channel_name},{link}\n")

def main():
    txt_urls_with_groups = [
        ("https://raw.githubusercontent.com/yuanzl77/IPTV/main/live.txt", ["å¤®è§†é¢‘é“", "å«è§†é¢‘é“","å½±è§†é¢‘é“"]),
        # å‡ºå¤„ æœˆå…‰å®ç›’æŠ“å–ç›´æ’­
        # ("https://ygbh.site/bh.txt", ["ğŸ’ä¸­å›½ç§»åŠ¨ITVğŸ‘‰ç§»åŠ¨"]),  # ä¿ç•™æ‰€æœ‰åˆ†ç»„
        ("https://raw.githubusercontent.com/zht298/IPTVlist/refs/heads/main/ygbh.txt", None),  # ç²¾ç®€æ’åºåçš„æœˆå…‰å®ç›’
        # å°è‹¹æœï¼Œèœ—ç‰›çº¿è·¯[æµ‹è¯•2]
        ("http://wp.wadg.pro/down.php/d7b52d125998d00e2d2339bac6abd2b5.txt", ["å¤®è§†é¢‘é“â‘ ", "ğŸ’å¤®è§†é¢‘é“", "å«è§†é¢‘é“â‘ ", "ğŸ“¡å«è§†é¢‘é“","éŸ©å›½é¢‘é“"]),      
        ("https://raw.githubusercontent.com/zht298/IPTVlist/main/dalian.txt", None),  # ä¿ç•™æ‰€æœ‰åˆ†ç»„  å¤§è¿å°
        # å‡ºå¤„ å°é¹¦é¹‰ç­‰å¤šå¤„è·å– 
        ("https://raw.githubusercontent.com/zht298/IPTVlist/main/JJdoudizhu.txt", None),  # ä¿ç•™æ‰€æœ‰åˆ†ç»„  JJæ–—åœ°ä¸»
        # å‡ºå¤„ https://adultiptv.net/â†’http://adultiptv.net/chs.m3u
        ("https://raw.githubusercontent.com/zht298/IPTVlist/main/chs.txt",None),  # ä¿ç•™æ‰€æœ‰åˆ†ç»„
    ]
    local_filenames_with_groups = []

    # æ­¥éª¤1ï¼šä¸‹è½½TXTæ–‡ä»¶
    for i, (url, groups) in enumerate(txt_urls_with_groups, start=1):
        local_filename = f"file{i}.txt"
        download_txt_file(url, local_filename)
        local_filenames_with_groups.append((local_filename, groups))

    # æ­¥éª¤2ï¼šåˆå¹¶TXTæ–‡ä»¶å¹¶è¿‡æ»¤
    output_filename = "merged_output.txt"
    max_channels_per_name = 10  # è®¾ç½®æ¯ä¸ªé¢‘é“åç§°æœ€å¤šä¿ç•™çš„é¡¹æ•°é‡
    merge_txt_files(local_filenames_with_groups, output_filename, max_channels_per_name)

if __name__ == "__main__":
    main()
